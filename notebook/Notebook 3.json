{
	"name": "Notebook 3",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "mypool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "fd4e8add-c99e-4a03-a0b5-170208930270"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7374728e-d82a-4420-8e42-c2f7ef16e4c3/resourceGroups/new_rp/providers/Microsoft.Synapse/workspaces/mywp7971/bigDataPools/mypool",
				"name": "mypool",
				"type": "Spark",
				"endpoint": "https://mywp7971.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/mypool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id\r\n",
					"my_dict = {'name': '\"', 'school': '//'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_book.csv\")\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"# Apply the split_record function to each record using map()\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"# Assuming you have an RDD split_rdd with records split by comma\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_rdd = split_rdd.filter(lambda row: row != header)\r\n",
					"df=spark.createDataFrame(data_without_rdd,header)\r\n",
					"df = df.withColumn('pri_id',monotonically_increasing_id())\r\n",
					"my_lis.append(df)\r\n",
					"\r\n",
					"def replace_chars_in_list(strings_list, char_to_replace):\r\n",
					"    modified_list = []\r\n",
					"    \r\n",
					"    for string in strings_list:\r\n",
					"        if isinstance(string,str):\r\n",
					"            modified_string = (string[:1]+string[1:-1].replcae(char_to_replace,'')+\r\n",
					"            string[-1:])\r\n",
					"            modified_list.append(modified_string)\r\n",
					"    return modified_list\r\n",
					"\r\n",
					"for key,value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        index = header.index(key)\r\n",
					"        modified_rdd = split_rdd.map(lambda x: x[:index]+[replace_chars_in_list([x[index]],value)[0]]+x[index +1:]) if len(x)>index else x)\r\n",
					"        data_wo_header1 = modified_rdd.filter(lambda row: row != header)\r\n",
					"        df_new = spark.createDataFrame(data_wo_header1,header)\r\n",
					"        df_new = df_new.withColumn('pri_id',monotonically_increasing_id())\r\n",
					"        my_lis.append(df_new)\r\n",
					"\r\n",
					"for i in my_lis:\r\n",
					"    i.show()\r\n",
					"\r\n",
					"final_df = my_lis[-1]\r\n",
					"for df in my_lis[-2::-1]:\r\n",
					"    lat_df = final_df.join(df,'pri_id','leftsemi')\r\n",
					"\r\n",
					"display(final_df)\r\n",
					"\r\n",
					""
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\r\n",
					"\r\n",
					"my_dict = {'name': '\"', 'school': '//'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_book.csv\")\r\n",
					"\r\n",
					"\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_header = split_rdd.filter(lambda row: row != header)\r\n",
					"df = spark.createDataFrame(data_without_header, header)\r\n",
					"df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"my_lis.append(df)\r\n",
					"\r\n",
					"\r\n",
					"def replace_chars_in_list(strings_list, char_to_replace):\r\n",
					"    modified_list = []\r\n",
					"\r\n",
					"    for string in strings_list:\r\n",
					"        if isinstance(string, str):\r\n",
					"            modified_string = string[:1] + string[1:-1].replace(char_to_replace, '') + string[-1]\r\n",
					"            modified_list.append(modified_string)\r\n",
					"    return modified_list\r\n",
					"\r\n",
					"\r\n",
					"for key, value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        index = header.index(key)\r\n",
					"        modified_rdd = split_rdd.map(lambda x: x[:index] + replace_chars_in_list([x[index]], value) + x[index + 1:] if len(x) > index else x)\r\n",
					"        data_wo_header1 = modified_rdd.filter(lambda row: row != header)\r\n",
					"        df_new = spark.createDataFrame(data_wo_header1, header)\r\n",
					"        df_new = df_new.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"        my_lis.append(df_new)\r\n",
					"\r\n",
					"for i in my_lis:\r\n",
					"    i.show()\r\n",
					"\r\n",
					"final_df = my_lis[-1]\r\n",
					"for df in my_lis[-2::-1]:\r\n",
					"    final_df = final_df.join(df, 'pri_id', 'leftsemi')\r\n",
					"\r\n",
					"final_df.show()\r\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id,col\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\r\n",
					"\r\n",
					"my_dict = {'name': '\"', 'school': '\\\\'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_issue.csv\")\r\n",
					"\r\n",
					"\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_header = split_rdd.filter(lambda row: row != header)\r\n",
					"df = spark.createDataFrame(data_without_header, header)\r\n",
					"df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"def replace_chars_in_list(strings_list, char_to_replace):\r\n",
					"    modified_list = []\r\n",
					"\r\n",
					"    for string in strings_list:\r\n",
					"        if isinstance(string, str):\r\n",
					"            modified_string = string[:1] + string[1:-1].replace(char_to_replace, '') + string[-1]\r\n",
					"            modified_list.append(modified_string)\r\n",
					"    return modified_list\r\n",
					"\r\n",
					"\r\n",
					"for key, value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        index = header.index(key)\r\n",
					"        modified_rdd = split_rdd.map(lambda x: x[:index] + replace_chars_in_list([x[index]], value) + x[index + 1:] if len(x) > index else x)\r\n",
					"        data_wo_header1 = modified_rdd.filter(lambda row: row != header)\r\n",
					"        df_new = spark.createDataFrame(data_wo_header1, header)\r\n",
					"        df_new = df_new.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"        df_new = df_new.select(col(key),col('pri_id'))\r\n",
					"        my_lis.append(df_new)\r\n",
					"\r\n",
					"\r\n",
					"keys_list = list(my_dict.keys())\r\n",
					"print(keys_list)\r\n",
					"columns_to_drop_objects = [col(col_name) for col_name in keys_list]\r\n",
					"print(columns_to_drop_objects)\r\n",
					"result_df = None\r\n",
					"for col_name in keys_list:\r\n",
					"    df = df.drop(col(col_name))\r\n",
					"my_lis.insert(0,df)\r\n",
					"for i in my_lis:\r\n",
					"    i.show(5)\r\n",
					"for df in my_lis:\r\n",
					"    if result_df is None:\r\n",
					"        result_df = df\r\n",
					"    else:\r\n",
					"        result_df = result_df.unionByName(df,allowMissingColumns=True)\r\n",
					"#final_df = my_lis[0]\r\n",
					"#for df in my_lis[1:]:\r\n",
					"    #final_df = final_df.join(df, 'pri_id', 'leftsemi')\r\n",
					"\r\n",
					"\r\n",
					"result_df.show()\r\n",
					""
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id, col\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\r\n",
					"\r\n",
					"my_dict = {'name': '\"', 'school': '\\\\'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_issue.csv\")\r\n",
					"\r\n",
					"\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_header = split_rdd.filter(lambda row: row != header)\r\n",
					"df = spark.createDataFrame(data_without_header, header)\r\n",
					"df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"#df.show()\r\n",
					"\r\n",
					"for key, value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        index = header.index(key)\r\n",
					"        modified_rdd = split_rdd.map(lambda x: x[:index] + [x[index].replace(value, '')] + x[index + 1:] if len(x) > index else x)\r\n",
					"        data_wo_header1 = modified_rdd.filter(lambda row: row != header)\r\n",
					"        df_new = spark.createDataFrame(data_wo_header1, header)\r\n",
					"        df_new = df_new.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"        df_new = df_new.select(col(key), col('pri_id'))\r\n",
					"        df_new.printSchema()\r\n",
					"        my_lis.append(df_new)\r\n",
					"\r\n",
					"columns_to_drop = list(my_dict.keys())\r\n",
					"\r\n",
					"for col_name in columns_to_drop:\r\n",
					"    df = df.drop(col_name)\r\n",
					"df.printSchema()\r\n",
					"my_lis.insert(0, df)\r\n",
					"#for i in my_lis:\r\n",
					"    #i.show(5)\r\n",
					"df_list = []\r\n",
					"result_df = None\r\n",
					"for df in my_lis:\r\n",
					"    \r\n",
					"    if result_df is None:\r\n",
					"        result_df = df\r\n",
					"    #else:\r\n",
					"        #result_df = result_df.unionByName(df,allowMissingColumns=True)\r\n",
					"    else:\r\n",
					"        final_df = result_df.join(df, 'pri_id', 'right')\r\n",
					"        df_list.append(final_df)\r\n",
					"        next_df = final_df\r\n",
					"        #next_df.show()\r\n",
					"'''my_df = None\r\n",
					"for j in df_list:\r\n",
					"    if my_df is None:\r\n",
					"        my_df = df\r\n",
					"    else:\r\n",
					"        my_df = my_df.unionByName(j,allowMissingColumns=True)'''\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"\r\n",
					"final_df.show()\r\n",
					""
				],
				"execution_count": 77
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"final_df.filter(final_df.pri_id == 0).show()"
				],
				"execution_count": 68
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"data1 = [(1,'piyush',200000),(2,'sakshi',600000),(2,'sakshi',600000),(3,'sakshi',800000),(4,'ram',400000)]\r\n",
					"data2 = [(9,'yash',200000),(2,'sakshi',600000),(2,'ruhi',600000),(7,'shawn',800000),(5,'prem',400000)]\r\n",
					"schema1 = ['id','name','salary']\r\n",
					"schema2 = ['id','name','new_salary']\r\n",
					"df_new1= spark.createDataFrame(data1,schema1)\r\n",
					"df_new2= spark.createDataFrame(data2,schema2)\r\n",
					"df_new1.show()\r\n",
					"df_new2.show()\r\n",
					"lat_df = df_new1.unionByName(df_new2,allowMissingColumns=True) #unionByName is used to merge/union two DFs of different dataframes\r\n",
					"lat_df.show()"
				],
				"execution_count": 64
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id, col\r\n",
					"\r\n",
					"spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\r\n",
					"\r\n",
					"my_dict = {'name': '\"', 'school': '\\\\'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_issue.csv\")\r\n",
					"\r\n",
					"\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_header = split_rdd.filter(lambda row: row != header)\r\n",
					"df = spark.createDataFrame(data_without_header, header)\r\n",
					"df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"my_lis.append(df)\r\n",
					"\r\n",
					"\r\n",
					"def replace_chars_in_list(string, char_to_replace):\r\n",
					"    if isinstance(string, str):\r\n",
					"        modified_string = string.replace(char_to_replace, '')\r\n",
					"        return modified_string\r\n",
					"    return string\r\n",
					"\r\n",
					"\r\n",
					"base_df = my_lis[0]\r\n",
					"\r\n",
					"for key, value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        df = base_df\r\n",
					"        index = header.index(key)\r\n",
					"        replace_func = col(header[index]).cast(\"string\").replace(value, \"\")\r\n",
					"        df = df.withColumn(header[index], replace_func)\r\n",
					"        df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"        my_lis.append(df)\r\n",
					"        base_df = base_df.join(df, 'pri_id', 'left')\r\n",
					"\r\n",
					"base_df.show()\r\n",
					""
				],
				"execution_count": 145
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"variable_names = []  # Initialize an empty list to store variable names\r\n",
					"\r\n",
					"num_variables = 5\r\n",
					"\r\n",
					"for i in range(1, num_variables + 1):\r\n",
					"    var_name = f\"var_{i}\"\r\n",
					"    variable_names.append(var_name)  # Append the variable name to the list\r\n",
					"    \r\n",
					"    # Create variables using exec\r\n",
					"    exec(f\"{var_name} = ''\")\r\n",
					"\r\n",
					"# Display the list containing variable names\r\n",
					"print(variable_names)\r\n",
					""
				],
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import monotonically_increasing_id, col\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"spark = SparkSession.builder.appName(\"DataFrameExample\").getOrCreate()\r\n",
					"\r\n",
					"my_dict = {'name': '\"', 'school': '\\\\'}\r\n",
					"my_lis = []\r\n",
					"original_rdd = sc.textFile(\"abfss://csvfile@myadls5434.blob.core.windows.net/csv_issue.csv\")\r\n",
					"\r\n",
					"\r\n",
					"def split_record(record):\r\n",
					"    return record.split(',')\r\n",
					"\r\n",
					"split_rdd = original_rdd.map(split_record)\r\n",
					"header = split_rdd.first()\r\n",
					"data_without_header = split_rdd.filter(lambda row: row != header)\r\n",
					"df = spark.createDataFrame(data_without_header, header)\r\n",
					"df = df.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"#df.show()\r\n",
					"\r\n",
					"for key, value in my_dict.items():\r\n",
					"    if key in header:\r\n",
					"        index = header.index(key)\r\n",
					"        modified_rdd = split_rdd.map(lambda x: x[:index] + [x[index].replace(value, '')] + x[index + 1:] if len(x) > index else x)\r\n",
					"        data_wo_header1 = modified_rdd.filter(lambda row: row != header)\r\n",
					"        df_new = spark.createDataFrame(data_wo_header1, header)\r\n",
					"        df_new = df_new.withColumn('pri_id', monotonically_increasing_id())\r\n",
					"        df_new = df_new.select(col(key), col('pri_id'))\r\n",
					"        df_new.printSchema()\r\n",
					"        my_lis.append(df_new)\r\n",
					"\r\n",
					"columns_to_drop = list(my_dict.keys())\r\n",
					"\r\n",
					"for col_name in columns_to_drop:\r\n",
					"    df = df.drop(col_name)\r\n",
					"fav_df = df\r\n",
					"fav_df.show()\r\n",
					"\r\n",
					"#df.printSchema()\r\n",
					"my_lis.insert(0, df)\r\n",
					"#for i in my_lis:\r\n",
					"    #i.show(5)\r\n",
					"df_list = []\r\n",
					"df_list.append(df)\r\n",
					"result_df = None\r\n",
					"boo = 0\r\n",
					"for df in my_lis:\r\n",
					"    \r\n",
					"    final_df = df_list[-1].join(df, 'pri_id', 'right')\r\n",
					"    df_list.append(final_df)\r\n",
					"        \r\n",
					"        #next_df.show()\r\n",
					"\r\n",
					"\r\n",
					"    \r\n",
					"\r\n",
					"final_df.show()\r\n",
					"\r\n",
					"#duplicate_cols = []\r\n",
					"#for i, col1 in enumerate(df.columns):\r\n",
					"    ##for col2 in df.columns[i + 1:]:\r\n",
					"        #if col1 != col2 and df.select(col1).subtract(df.select(col2)).count() == 0:\r\n",
					"            #duplicate_cols.append(col2)'''\r\n",
					"\r\n",
					"# Drop duplicate columns\r\n",
					"#df4 = neee_df.drop(*duplicate_cols)\r\n",
					"\r\n",
					"# Show the resulting DataFrame after dropping duplicate columns\r\n",
					"#df4.show()\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 186
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"neee_df = df_list[-1]\r\n",
					"dis_list = []\r\n",
					"exists_list =[]\r\n",
					"for i in neee_df.columns:\r\n",
					"    if i not in dis_list:\r\n",
					"        dis_list.append(i)\r\n",
					"\r\n",
					"    else:\r\n",
					"        exists_list.append(i)\r\n",
					"\r\n",
					"print(dis_list)\r\n",
					"print(exists_list)"
				],
				"execution_count": 187
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#neee_df.show()\r\n",
					"doo =[]\r\n",
					"doo1 = []\r\n",
					"boo =[]\r\n",
					"boo1=[]\r\n",
					"index_list=neee_df.columns\r\n",
					"for i  in neee_df.columns:\r\n",
					"    \r\n",
					"    if i  not in exists_list:\r\n",
					"        doo.append(i)\r\n",
					"        doo1.append(index_list.index(i))\r\n",
					"    else:\r\n",
					"        boo.append(i)\r\n",
					"        boo1.append(index_list.index(i))\r\n",
					"\r\n",
					"print(doo1)\r\n",
					"print(doo)\r\n",
					"print(boo)\r\n",
					"print(boo1)\r\n",
					"ne_jk=[]\r\n",
					"boat=0\r\n",
					"half_index = len(boo1) // 2\r\n",
					"\r\n",
					"# Slice the original list to get values until the half index and store in another list\r\n",
					"ne_jk = boo1[:half_index]\r\n",
					"\r\n",
					"print(ne_jk)\r\n",
					"\r\n",
					"\r\n",
					""
				],
				"execution_count": 188
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"'''#indices_to_drop = ne_jk # Example: Dropping columns at index positions 1 and 3\r\n",
					"\r\n",
					"# Get all column names\r\n",
					"columns_to_drop_indices = [1,2,3]  # For example, dropping columns at indices 1 and 3 (age and gender)\r\n",
					"\r\n",
					"# Check if indices are valid\r\n",
					"if any(idx >= len(neee_df.columns) for idx in columns_to_drop_indices):\r\n",
					"    print(idx)\r\n",
					"    print(\"Invalid column index provided.\")\r\n",
					"else:\r\n",
					"    # Get the list of column names to drop\r\n",
					"    columns_to_drop = [df.columns[i] for i in columns_to_drop_indices]\r\n",
					"    \r\n",
					"    # Drop columns based on their indices\r\n",
					"    df = neee_df.drop(*columns_to_drop)\r\n",
					"\r\n",
					"    # Show the resulting DataFrame after dropping columns'''\r\n",
					"    #df.show()"
				],
				"execution_count": 189
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"#neee_df.show()\r\n",
					"columns_to_drop_indices = [1,2,3]  # For example, dropping columns at indices 1 and 3 (age and gender)\r\n",
					"\r\n",
					"# Check if indices are valid\r\n",
					"if any(idx >= len(neee_df.columns) for idx in columns_to_drop_indices):\r\n",
					"    \r\n",
					"    print(\"Invalid column index provided.\")\r\n",
					"else:\r\n",
					"    # Get the list of column names to drop\r\n",
					"    columns_to_drop = [neee_df.columns[i] for i in columns_to_drop_indices]\r\n",
					"    \r\n",
					"    # Drop columns based on their indices\r\n",
					"    neee_df = neee_df.drop(*columns_to_drop)\r\n",
					"    neee_df.show()\r\n",
					"    fav_df.show()\r\n",
					"\r\n",
					"    # Show the resulting DataFrame after dropping columns'''\r\n",
					"    join_df= neee_df.join(fav_df,'pri_id','right')\r\n",
					"    join_df.show()"
				],
				"execution_count": 190
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}